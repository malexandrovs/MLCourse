{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "00a235fd0042bf5bc7946d98643a4171",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2019) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e65a504c1df06d5929d7da5fc5dde8db",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d59e79abac4f1cc5fa7b3651c8cdaeef",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 26, 2019**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "feb5181e3a6128a0475a359eb92fb96a",
     "grade": false,
     "grade_id": "cell-78e418c8c7c6b9cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Expectation and Variance) [2 Points]\n",
    "\n",
    "This exercise is supposed to be very easy but in this and the following sheets we will give points. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7bfb3ab0f16512e07682e92e1803b028",
     "grade": false,
     "grade_id": "math-stat-q1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**a)** What is the relation between sample mean and population mean (expectated value)? How to compute it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "bb830ac5e87551c43f910d26785877da",
     "grade": true,
     "grade_id": "math-stat-a1",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "250b2d21a6789689157350ec662a25b8",
     "grade": false,
     "grade_id": "math-stat-q2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**b)** What is the variance? What does it express? Why is there a square in the formula? What is biased and unbiased sample variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "7129fb0ad56a0b7eec54d6f0b7d2af15",
     "grade": true,
     "grade_id": "math-stat-a2",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8c8eaf05fddc7674d302e03621cc257d",
     "grade": false,
     "grade_id": "math-stat-q3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**c)** Can you compute mean and variance of a given 1-dimensional dataset (e.g., $D=\\{9,10,11,7,13\\}$). Can you do the same for a 3-dimensional dataset (e.g., D=\\{(1,10,9), (1,10,10), (10,10,11), (19,10,7), (19,10,13)\\})?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "5940d1bd11b3590fad0bf8885cf7f533",
     "grade": true,
     "grade_id": "math-stat-a3",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Yes I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "24ac2621e95ff51276b904a029d1d3c3",
     "grade": true,
     "grade_id": "math-stat-i3",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4., 10., 10.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1 = np.array([9, 10, 11, 7, 13])\n",
    "D1.mean()\n",
    "\n",
    "D3 = np.array([[1, 10, 9], [1, 10, 10], [10, 10, 11]])\n",
    "D3.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b0d30ecc8f568c37225ca44895864f4e",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: The Perceptron [3 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1b5db7a1affd1b789fb4d2ae03d1370d",
     "grade": false,
     "grade_id": "ex1a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) The Logic Perceptron\n",
    "\n",
    "For the following two logical functions sketch a perceptron's weights after it was trained. To do so, figure out when the perceptron should fire. Then come up with ideas of how you can achieve this. Remember that $w_0$, the bias, is used as a threshold and that there is a constant $x_0 = 1$. Provide the values for $w_0,w_1,w_2$ as well as some explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6aa1df3a8651c3c9427770f43a910183",
     "grade": false,
     "grade_id": "ex1a1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1) $(A \\wedge B) \\vee (\\neg A \\wedge B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "43bf5abdf97332abb608fa4f463e0096",
     "grade": true,
     "grade_id": "ex1a1_solution",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c83d94ed87766472462291798c665a12",
     "grade": false,
     "grade_id": "ex1a2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2) $(A \\wedge B) \\vee (\\neg A \\wedge B) \\vee (A \\wedge \\neg B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bc15fc82fce83e9b486124240adbf434",
     "grade": true,
     "grade_id": "ex1a2_solution",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcbb7699963c6cd5bb94b5dbfaee2f10",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Perceptron [7 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "147e17ff61ed42d7514ce9820f6d6ac4",
     "grade": false,
     "grade_id": "ex2_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this exercise you will implement a simple perceptron as described in the lecture [ML-07 Slide 31]. As with  previous exercises it is possible to not use our premade code blocks but write the single Perceptron completely from scratch (an empty cell to do so can be found [below](#Own-Implementation)). \n",
    "\n",
    "Use the following output function:\n",
    "$$y = \\begin{cases}1 \\quad \\text{if} \\ s > 0\\\\0 \\quad \\text{else}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "665667ab36a90604ca8cb5fd3fed0b73",
     "grade": false,
     "grade_id": "ex2_x3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The `TODO`'s in the following code segments guide you through what has to be done.\n",
    "\n",
    "*Hint*: If you have problems with `np.arrays` (which usually have shapes like `(13,)`, thus with one degenerate dimension, either set the shapes manually (`my_np_array.shape = (13, 1)`). Other useful functions might be `np.append` or `np.hstack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9f6940ce6d3774e5b825c6a10e74d48b",
     "grade": true,
     "grade_id": "ex2_a",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "# TODO: Write the input activation (called net_input) and the output function (called out_fun).\n",
    "def net_input(D, W):\n",
    "    return np.dot(D, W)\n",
    "\n",
    "def out_fun(val):\n",
    "    return np.heaviside(val,0)\n",
    "\n",
    "# TODO: Write a function generate_weights that generates N (= number of dimensions) + 1 (w_0) random weights.\n",
    "def generate_weights(N):\n",
    "    return np.hstack((1, rnd.rand(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Testing the perceptron with a concrete example ##\n",
    "####################################################\n",
    "\n",
    "# Dimensions for our test.\n",
    "dims = 12\n",
    "\n",
    "# Input is a row vector. (Shape is (1, 13).)\n",
    "D = np.hstack((1, rnd.rand(dims) - 0.5))\n",
    "\n",
    "# Weights are stored in a vector.\n",
    "W = generate_weights(dims)\n",
    "\n",
    "out = out_fun(net_input(D, W))\n",
    "\n",
    "assert out == 1 or out == 0, \"The output has to be either 1 or 0, but was {}\".format(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "16ab5b42b5f3dbbc919f6db49591a0f9",
     "grade": false,
     "grade_id": "ex2b_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following `eval_network(t, D, W)` function is used to measure the performance of your perceptron for the upcoming task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_network(t, D, W):\n",
    "    \"\"\"\n",
    "    This function takes the trained weights of a perceptron\n",
    "    and the input data (D) as well as the correct target values (t)\n",
    "    and computes the overall error rate of the perceptron.\n",
    "    \"\"\"\n",
    "    error = 0.0\n",
    "    size = max(D.shape)\n",
    "    for i in range(size):\n",
    "        out = out_fun(net_input(D[i], W))\n",
    "        error = error + abs(t[i] - out)\n",
    "    # Normalize the error.\n",
    "    try:\n",
    "        return error.item(0) / size\n",
    "    except AttributeError:\n",
    "        return error / size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a4f76f6451302a0efb9ac6ff6ea7e181",
     "grade": false,
     "grade_id": "ex2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will use the above defined functions to train the perceptron to one of the following logical functions: OR, NAND or NOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def functions_to_learn(selector):\n",
    "        \"\"\"\n",
    "        Functional definitions for the perceptron to learn\n",
    "        Instantiates plots for visualization of the decision boundary\n",
    "        :param selector: selects which function to activate\n",
    "        :return function:\n",
    "        \"\"\"\n",
    "        plot_points = [[0,0],[0,1],[1,0],[1,1]]\n",
    "        plot_colors = []\n",
    "        if selector == 'and':\n",
    "            function = lambda x1, x2: x1 and x2\n",
    "            for point in plot_points:\n",
    "                plot_colors.append(function(point[0], point[1]))\n",
    "            for color, point in enumerate(plot_points):\n",
    "                plt.scatter(*point, s=50, c='b' if plot_colors[color] == 1 else 'r')\n",
    "            print(\"Perceptron will now learn '{}'...\\n\\n\".format(selector))\n",
    "            return function\n",
    "        elif selector == 'or':\n",
    "            function = lambda x1, x2: x1 or x2\n",
    "            for point in plot_points:\n",
    "                plot_colors.append(function(point[0], point[1]))\n",
    "            for color, point in enumerate(plot_points):\n",
    "                plt.scatter(*point, s=50, c='b' if plot_colors[color] == 1 else 'r')\n",
    "            print(\"Perceptron will now learn '{}'...\\n\\n\".format(selector))\n",
    "            return function\n",
    "        elif selector == 'nand':\n",
    "            function = lambda x1, x2: not (x1 and x2)\n",
    "            for point in plot_points:\n",
    "                plot_colors.append(function(point[0], point[1]))\n",
    "            for color, point in enumerate(plot_points):\n",
    "                plt.scatter(*point, s=50, c='b' if plot_colors[color] == 1 else 'r')\n",
    "            print(\"Perceptron will now learn '{}'...\\n\\n\".format(selector))\n",
    "            return function\n",
    "        elif selector == 'nor':\n",
    "            function = lambda x1, x2: not (x1 or x2)\n",
    "            for point in plot_points:\n",
    "                plot_colors.append(function(point[0], point[1]))\n",
    "            for color, point in enumerate(plot_points):\n",
    "                plt.scatter(*point, s=50, c='b' if plot_colors[color] == 1 else 'r')\n",
    "            print(\"Perceptron will now learn '{}'...\\n\\n\".format(selector))\n",
    "            return function\n",
    "        elif selector == \"xor\":\n",
    "            function = lambda x1, x2: (x1 and not x2) or (not x1 and x2)\n",
    "            for point in plot_points:\n",
    "                plot_colors.append(function(point[0], point[1]))\n",
    "            for color, point in enumerate(plot_points):\n",
    "                plt.scatter(*point, s=50, c='b' if plot_colors[color] == 1 else 'r')\n",
    "            print(\"Perceptron will now learn '{}'...\\n\\n\".format(selector))\n",
    "            return function\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect function to learn.  Pick and/or/nand/nor.  Input was:\", selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e89199c7df979c735a9892c485e4ddb9",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron will now learn 'nand'...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall error of the Perceptron: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###################################################\n",
    "## Now we train our perceptron! [ML-07 Slide 33] ##\n",
    "###################################################\n",
    "\n",
    "# TODO: Write the update function (name it 'delta_fun')\n",
    "#       for the weights dependent on epsilon, the target,\n",
    "#       the output and the input vector.\n",
    "def delta_fun(eps, t, y, i):\n",
    "    su = (t-y)*i\n",
    "    return eps*su\n",
    "    \n",
    "\n",
    "# TODO: Define suitable parameters for your problem.\n",
    "# Use the following names:\n",
    "#   Ïµ: learning rate\n",
    "#   dims: dimensions\n",
    "#   training_size: the number of training samples\n",
    "Îµ = 0.03\n",
    "dims = 2\n",
    "training_size = 100\n",
    "\n",
    "\n",
    "# TODO: Generate the weights (in a variable called W).\n",
    "W = generate_weights(dims)\n",
    "\n",
    "# TODO: Generate a matrix D of truthvalue pairs.\n",
    "# The shape should be (training_size, dims).\n",
    "D = np.random.randint(0, 2, (training_size, dims))\n",
    "\n",
    "# TODO: Pad the input D with ones for the bias. The bias should always be\n",
    "# w_0, i. e. the first column of the data should be ones.\n",
    "bias = np.ones((training_size,1))\n",
    "#print(bias)\n",
    "D = np.append(bias,D, axis=1)\n",
    "# Learn one of the logical functions OR, NAND, NOR\n",
    "# (the lambda keyword is just a short way to define functions. It is equivalent to\n",
    "#   def log_operator_and(x1, x2):\n",
    "#       return x1 and x2\n",
    "log_operator_and  = lambda x1, x2: x1 and x2\n",
    "log_operator_or   = lambda x1, x2: x1 or x2\n",
    "log_operator_nand = lambda x1, x2: not (x1 and x2)\n",
    "log_operator_nor  = lambda x1, x2: not (x1 or x2)\n",
    "log_operator_xor  = lambda x1, x2: (x1 and not x2) or (not x1 and x2)\n",
    "\n",
    "# Change these two lines to choose the other operators:\n",
    "log_operator = log_operator_nand\n",
    "functions_to_learn(\"nand\")\n",
    "\n",
    "row_operator = lambda row: log_operator(row[0], row[1])\n",
    "labels = np.apply_along_axis(row_operator, 1, D[:, 1:])\n",
    "\n",
    "epochs = 200    # Extra question: What effects do changes in the epochs \n",
    "samp_size = 5  #                 and sample sizes have on our training?\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Sample random from the training data.\n",
    "    for idx in rnd.choice(range(training_size), samp_size, replace=False):\n",
    "        y = out_fun(net_input(D[idx], W))\n",
    "        W += delta_fun(Ïµ, labels[idx], y, D[idx])\n",
    "    # Plotting code\n",
    "    y_point = (0, (-W[0] / W[2]))\n",
    "    x_point = ((-W[0] / W[1]), 0)\n",
    "    try:\n",
    "        slope = (y_point[1] - x_point[1]) / (y_point[0] - x_point[0]) # will not work if x and y intercepts are 0\n",
    "    except ZeroDivisionError:\n",
    "        print(\"X and Y intercepts are both zero.  Due to the way slope is calculated, this causes a division by zero.  Sorry.\")\n",
    "    y_out = lambda points: slope * points\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    plt.plot(x, y_out(x) + y_point[1], 'g--', linewidth=3, alpha=i/epochs +.2 if i/epochs +.2 < 1 else 1)\n",
    "    \n",
    "plt.ylim([-.2, 1.2])\n",
    "plt.xlim([-.2, 1.2])\n",
    "plt.title(\"Logic Perceptron (Blue=True)\")\n",
    "plt.xlabel(\"True(1) or False(0)\")\n",
    "plt.ylabel(\"True(1) or False(0)\")\n",
    "plt.show()\n",
    "\n",
    "# Print the overall performance of the Perceptron.\n",
    "print(\"Overall error of the Perceptron: {:.2%}\".format(eval_network(labels, D, W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "16493889c4f652d2c56fa9a2decf9934",
     "grade": false,
     "grade_id": "ex2_x4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Own Implementation\n",
    "\n",
    "Skip this if you already implemented the perceptron above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b5fc95e3820d69a8543d181f934789f",
     "grade": true,
     "grade_id": "ex2_x4_solution",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Space for complete own implementation\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1796c6b2bab519f1d0f2575dce75712d",
     "grade": false,
     "grade_id": "ex3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Sigmoid Activation & Backpropagation Delta Functions [6 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a745c73103e3c96af0ca74cee47e41e",
     "grade": false,
     "grade_id": "ex3_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this exercise we are first going to take the derivative of a famous activation function - the sigmoid function:\n",
    "\n",
    "$$\\sigma(t)=\\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "This function is commonly used because of its nice analytical properties: Its domain is $\\in[0,1]$, it is non-linear, strictly monotonous, continuous, differentiable and the derivative can be expressed in terms of the original function at the given point. This allows us to avoid redundant calculations. The sigmoid function is a special case of the more general *Logistic function* which can be found in many different fields: Biology, chemistry, economics, demography and recently most prominently: artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "922739bef1543026c79b6f17f4ae5748",
     "grade": false,
     "grade_id": "ex3a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Take the derivative $\\frac{\\partial \\sigma}{\\partial t}$ and (if possible) write the resulting expression in terms of $\\sigma(t)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "70a6c8d0e0cac2195da0071b6250cb87",
     "grade": true,
     "grade_id": "ex3a_solution",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "$$\\sigma(t)' = e^{-t}*\\sigma(t)^2* \\frac{\\partial t}{\\partial t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "694ada0c37833d1cec551e4b661eb576",
     "grade": false,
     "grade_id": "ex3b_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Multilayer perceptrons (MLPs) can be regarded as a simple concatenation (and parallelization) of several perceptrons, each having a specified activation function $\\sigma$ and a set of weights $\\mathbf{w}_{ij}$. The idea that this can be done was discovered early after the invention of the perceptron, but people didn't really use it in practice because nobody really knew how to figure out the appropriate $\\mathbf{w}_{ij}$. The solution to this problem was the discovery of the backpropagation algorithm which consists of two steps: first propagating the input forward through the layers of the MLP and storing the intermediate results and then propagating the error backwards and adjusting the weights of the units accordingly.\n",
    "\n",
    "An updating rule for the output layer can be derived straightforward. The rules for the intermediate layers can be derived very similarly and only require a slight shift in perspective - the mathematics for that are however not in the standard toolkit so we are going to omit the calculations and refer you to the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "14c758f0d8d4e8349dc2d7539ca26e70",
     "grade": false,
     "grade_id": "ex3b_task",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We take the least-squares approach to derive the updating rule, i.e. we want to minimize the Loss function\n",
    "$$L = \\frac{1}{2}(y-t)^2$$\n",
    "where t is the given (true) label from the dataset and y is the (single) output produced by the MLP. To find the weights that minimize this expression we want to take the derivative of $L$ w.r.t. $\\mathbf{w}_{i}$ where we are now going to assume that the $\\mathbf{w}_{i}$ are the ones directly before the output layer:\n",
    "$$y = \\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)$$\n",
    "Calculate $\\frac{\\partial L}{\\partial \\mathbf{w}_{i}}$.\n",
    "\n",
    "*Hint*: Start here if you don't know what to do: $\\frac{\\partial L}{\\partial \\mathbf{w}_{i}} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial \\mathbf{w}_{i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9382266d0747e829a66ce66f548b2a23",
     "grade": true,
     "grade_id": "ex3b_solution",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "$n = \\sum_{k=1}^n \\mathbf{w}_{k}o_k$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\mathbf{w}_{i}} = (\\sigma(n)-t)e^{-n}\\sigma(n)^2o_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a71102ebcc5bca26278bc22ac7053aaa",
     "grade": false,
     "grade_id": "ex4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 4: The Tensorflow Playground [2 Points]\n",
    "\n",
    "\n",
    "With the online tool [TensorFlow playground](http://playground.tensorflow.org/) it is possible to interactively play with feed forward neural networks, compellingly visualize their behavior and share specific configurations. \n",
    "\n",
    "Follow [this link](http://playground.tensorflow.org/#activation=sigmoid&batchSize=1&dataset=gauss&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=1&seed=0.56339&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&learningRate_hide=false&regularizationRate_hide=true&percTrainData_hide=true&batchSize_hide=true&dataset_hide=false&regularization_hide=true&discretize_hide=true&stepButton_hide=false&showTestData_hide=false&problem_hide=true&noise_hide=true&activation_hide=true) to the TensorFlow playground. If you click it, many features are disabled and set to useful defaults, since they were either not discussed yet in the lecture or are not important for this exercise.\n",
    "\n",
    "You will see a simple configuration: Two activated inputs ($x_1$ and $x_2$), one hidden layer with one neuron (which can be understood as a simple perceptron) and the output shown as a nice picture. It initially shows a training loss of 0.527. Try and run it to see how the perceptron can learn to separate the two clusters. Note that for the rest of the exercise we assume at most about 1000 learning steps (usually many fewer will do it), so don't wait too long in front of your browser.\n",
    "\n",
    "The dataset gets fully classified after very few iterations. Next try the XOR dataset, either by clicking on it on the left (the top right data pattern) or by following [this link](http://playground.tensorflow.org/#activation=sigmoid&batchSize=1&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=1&seed=0.56339&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&learningRate_hide=false&regularizationRate_hide=true&percTrainData_hide=true&batchSize_hide=true&dataset_hide=false&regularization_hide=true&discretize_hide=true&stepButton_hide=false&showTestData_hide=false&problem_hide=true&noise_hide=true&activation_hide=true). You will notice that you won't achieve much better results than a loss of 0.4, which is just above chance. Try to improve the result by adding neurons and or layers (but don't change the inputs!) until you get a classification with a loss smaller than 0.01. You may also change the learning rate. Then copy the link from your browser address bar and paste it below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6650b5e876cd05f2c6caa2554ba6051d",
     "grade": true,
     "grade_id": "ex4a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "http://playground.tensorflow.org/#activation=sigmoid&batchSize=1&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=2,2&seed=0.56339&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&learningRate_hide=false&regularizationRate_hide=true&percTrainData_hide=true&batchSize_hide=true&dataset_hide=false&regularization_hide=true&discretize_hide=true&stepButton_hide=false&showTestData_hide=false&problem_hide=true&noise_hide=true&activation_hide=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "25ed18066ba4fd1c502a2225fbfd0b48",
     "grade": false,
     "grade_id": "ex4b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "How many neurons in hidden layers are already sufficient to get at least 99% classification (i.e. loss < 0.01) if they are a) in one hidden layer or b) in two hidden layers? You may consider configurations which just need above 1000 iterations to get there as well, but we don't expect you to run any configuration longer than 1000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e956d88bb9fa064ebdadbd48bc1b45c8",
     "grade": true,
     "grade_id": "ex4b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "a) you need at least 4 with learning rate decay and luck\n",
    "b) you need 2 per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
